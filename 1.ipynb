{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function1 ---> connect to buckect and retrieve the objects from the bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s3://rand1/hello.parquet']\n",
      "   hello\n",
      "0  world\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "bucket = 'rand1'\n",
    "s3_client = boto3.client(\"s3\")\n",
    "with open(\"./load_test_db/hello_test.parquet\", \"rb\") as f:\n",
    "    s3_client.upload_fileobj(f, f\"{bucket}\", \"hello.parquet\")\n",
    "bucket_objects = s3_client.list_objects_v2(Bucket = bucket)\n",
    "assert bucket_objects['KeyCount'] == 1\n",
    "file_paths = [f's3://{bucket}/{obj[\"Key\"]}' for obj in bucket_objects['Contents']]\n",
    "print(file_paths)\n",
    "for file_path in file_paths:\n",
    "    s3_file = pq.ParquetDataset(file_path)\n",
    "    table = s3_file.read().to_pandas()\n",
    "    df= pd.DataFrame(table)\n",
    "    print(df)\n",
    "assert df.columns.tolist() == ['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "def retrive_files(bucket):\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    bucket = s3_client.list_objects_v2(Bucket = bucket)\n",
    "    return bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 1.25s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "from moto import mock_s3\n",
    "import pytest\n",
    "import pyarrow.parquet as pq\n",
    "import pg8000\n",
    "from pyarrow import fs\n",
    "\n",
    "@pytest.fixture(scope='function')\n",
    "def aws_credentials():\n",
    "    \"\"\"Mocked AWS Credentials for moto.\"\"\"\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = 'test'\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = 'test'\n",
    "    os.environ['AWS_SECURITY_TOKEN'] = 'test'\n",
    "    os.environ['AWS_SESSION_TOKEN'] = 'test'\n",
    "    os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
    "\n",
    "@mock_s3\n",
    "def test_creating_mock_s3():\n",
    "    conn = boto3.client(\"s3\", region_name=\"us-east-1\")\n",
    "    res = conn.create_bucket(Bucket=\"test_bucket_29\")\n",
    "    assert res['ResponseMetadata']['HTTPStatusCode'] == 200\n",
    "\n",
    "@mock_s3\n",
    "def test_accessing_the_objects_in_the_bucket():\n",
    "    conn = boto3.client(\"s3\", region_name=\"us-east-1\")\n",
    "    res = conn.create_bucket(Bucket=\"test_bucket_29\")\n",
    "    parquet_files = retrive_files('test_bucket_29')\n",
    "    assert parquet_files['KeyCount'] == 0\n",
    "\n",
    "@mock_s3\n",
    "def test_retreiving_objects_in_the_bucket():\n",
    "    bucket = \"test_bucket_29\"\n",
    "    conn = boto3.client(\"s3\")\n",
    "    res = conn.create_bucket(Bucket=bucket)\n",
    "    parquet_files = retrive_files(bucket)\n",
    "    # uploading some fake data to bucket\n",
    "    with open(\"./load_test_db/hello_test.parquet\", \"rb\") as f:\n",
    "        conn.upload_fileobj(f, f\"{bucket}\", \"hello.parquet\")\n",
    "    parquet_files = retrive_files(bucket)\n",
    "    assert parquet_files['KeyCount'] == 1\n",
    "    \n",
    "@pytest.fixture\n",
    "def test_retreiving_objects_in_the_bucketss():\n",
    "    bucket =\"rand2\"\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    res = s3_client.create_bucket(Bucket=bucket,CreateBucketConfiguration={'LocationConstraint': 'eu-west-2'})\n",
    "    with open(\"./load_test_db/hello_test.parquet\", \"rb\") as f:\n",
    "        s3_client.upload_fileobj(f, f\"{bucket}\", \"hello.parquet\")\n",
    "    bucket_objects = retrive_files(bucket)\n",
    "    file_paths = [f's3://{bucket}/{obj[\"Key\"]}' for obj in bucket_objects['Contents']]\n",
    "    for file_path in file_paths:\n",
    "        s3_file = pq.ParquetDataset(file_path)\n",
    "        table = s3_file.read().to_pandas()\n",
    "        df= pd.DataFrame(table)\n",
    "    assert df.columns.tolist() == ['hello']\n",
    "\n",
    "# @pytest.fixture\n",
    "# def test_warehouse_connection():\n",
    "#     connect = make_warehouse_connection()\n",
    "#     assert isinstance(connect, pg8000.Connection)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pg8000.legacy.Connection object at 0x7fbc667cebe0>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from os.path import join, dirname\n",
    "from pathlib import Path\n",
    "import pg8000\n",
    "def make_warehouse_connection():\n",
    "    # dotenv_path = join(dirname(__file__), '../config/.env.data_warehouse')\n",
    "    dotenv_path = Path('./config/.env.data_warehouse')\n",
    "    load_dotenv(dotenv_path)\n",
    "    API_HOST =  os.environ[\"host\"]\n",
    "    API_USER = os.environ[\"user\"]\n",
    "    API_PASS = os.environ[\"password\"]\n",
    "    API_DBASE = os.environ[\"database\"]\n",
    "    conn = pg8000.connect(\n",
    "        host=API_HOST,\n",
    "        user=API_USER,\n",
    "        password=API_PASS,\n",
    "        database=API_DBASE\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "print(make_warehouse_connection())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pg8000\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def get_data(bucket_name, file_path):\n",
    "    s3 = boto3.client('s3')\n",
    "    objects = s3.list_objects_v2(Bucket=bucket_name, Prefix=file_path)['Contents']\n",
    "    dfs = {}\n",
    "    for obj in objects:\n",
    "        key = obj['Key']\n",
    "        filename = key.split('/')[-1].split('.')[0]\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        buffer = io.BytesIO(obj['Body'].read())\n",
    "        table = pq.read_table(buffer)\n",
    "        df = table.to_pandas()\n",
    "        dfs[f\"df_{filename}\"] = df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def make_warehouse_connection(dotenv_path=\"./config/.env.data_warehouse\"):\n",
    "    try:\n",
    "        dotenv = Path(dotenv_path)\n",
    "        load_dotenv(dotenv)\n",
    "        API_HOST = os.environ[\"host\"]\n",
    "        API_USER = os.environ[\"user\"]\n",
    "        API_PASS = os.environ[\"password\"]\n",
    "        API_DBASE = os.environ[\"database\"]\n",
    "        conn = pg8000.connect(\n",
    "            host=API_HOST,\n",
    "            user=API_USER,\n",
    "            password=API_PASS,\n",
    "            database=API_DBASE\n",
    "        )\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to the data warehouse: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7B3669CW6W3WVNZS',\n",
       "  'HostId': 'qnp9SVonGE9TTVzobTtAR7voAjckgowSm9WQvI5d/pisFkNuxt8wVqv9G2c6raqyLY0CZrb3FKM=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'qnp9SVonGE9TTVzobTtAR7voAjckgowSm9WQvI5d/pisFkNuxt8wVqv9G2c6raqyLY0CZrb3FKM=',\n",
       "   'x-amz-request-id': '7B3669CW6W3WVNZS',\n",
       "   'date': 'Fri, 31 Mar 2023 08:51:28 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"4f231988a0ecfb93deda19f91db2bf81\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"4f231988a0ecfb93deda19f91db2bf81\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "bucket_name = 'rand6'\n",
    "file_path = 'data/parquet'\n",
    "dotenv_path = './config/.env.test'\n",
    "s3_client= boto3.resource('s3')\n",
    "res = s3_client.create_bucket(Bucket=bucket_name)\n",
    "s3_client.Object(bucket_name, f\"{file_path}/dim_currency.parquet\").put(\n",
    "        Body=open('./load_test_db/dim_currency.parquet', 'rb'))\n",
    "s3_client.Object(bucket_name, f\"{file_path}/dim_date.parquet\").put(\n",
    "        Body=open('./load_test_db/dim_date.parquet', 'rb'))\n",
    "s3_client.Object(bucket_name, f\"{file_path}/hello.parquet\").put(\n",
    "        Body=open('./load_test_db/hello_test.parquet', 'rb'))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(bucket_name,file_path)\n",
    "conn = make_warehouse_connection(dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading table dim_currency\n",
      "Loading table dim_date\n"
     ]
    }
   ],
   "source": [
    "cur = conn.cursor()\n",
    "for table in df:\n",
    "    table_name = table[3:]\n",
    "    print(f\"Loading table {table_name}\")\n",
    "    for index, row in df[table].iterrows():\n",
    "        values = ', '.join(['%s'] * len(row))\n",
    "        columns = ', '.join(row.index)\n",
    "        sql = f\"INSERT INTO {table_name} ({columns}) VALUES ({values})\"\n",
    "        cur.execute(sql, tuple(row))\n",
    "conn.commit()\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Records': [{'eventVersion': '2.0', 'eventSource': 'aws:s3', 'awsRegion': 'us-east-1', 'eventTime': '1970-01-01T00:00:00.000Z', 'eventName': 'ObjectCreated:Put', 'userIdentity': {'principalId': 'EXAMPLE'}, 'requestParameters': {'sourceIPAddress': '127.0.0.1'}, 'responseElements': {'x-amz-request-id': 'EXAMPLE123456789', 'x-amz-id-2': 'EXAMPLE123/5678abcdefghijklambdaisawesome/mnopqrstuvwxyzABCDEFGH'}, 's3': {'s3SchemaVersion': '1.0', 'configurationId': 'testConfigRule', 'bucket': {'name': 'wrong_bucket', 'ownerIdentity': {'principalId': 'EXAMPLE'}, 'arn': 'arn:aws:s3:::example-bucket'}, 'object': {'key': 'data/parquet/dim_date.parquet', 'size': 1024, 'eTag': '0123456789abcdef0123456789abcdef', 'sequencer': '0A1B2C3D4E5F678901'}}}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('./tests/test_data/incorrect_bucket.json') as i:\n",
    "    json=json.loads(i.read())\n",
    "print (json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
